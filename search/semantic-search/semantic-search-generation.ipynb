{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W390nLBmZlId"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/semantic-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/semantic-search.ipynb)\n",
        "\n",
        "# Semantic Search\n",
        "\n",
        "[![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/fast-link.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/semantic-search-fast.ipynb)\n",
        "\n",
        "In this walkthrough we will see how to use Pinecone for semantic search. To begin we must install the required prerequisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q03L1BYEZQfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone-datasets==0.5.0rc2\n",
            "  Using cached pinecone_datasets-0.5.0rc2-py3-none-any.whl (12 kB)\n",
            "Collecting datasets==2.12.0\n",
            "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
            "Collecting fsspec<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "Collecting gcsfs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached gcsfs-2023.5.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting pandas<3.0.0,>=2.0.0 (from pinecone-datasets==0.5.0rc2)\n",
            "  Downloading pandas-2.0.2-cp39-cp39-macosx_11_0_arm64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-client==3.0.0rc2 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached pinecone_client-3.0.0rc2-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (6.6 MB)\n",
            "Collecting polars<0.17.0,>=0.16.4 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached polars-0.16.18-cp37-abi3-macosx_11_0_arm64.whl (13.9 MB)\n",
            "Collecting protobuf<3.20.0,>=3.19.3 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
            "Collecting pyarrow<13.0.0,>=12.0.0 (from pinecone-datasets==0.5.0rc2)\n",
            "  Downloading pyarrow-12.0.0-cp39-cp39-macosx_11_0_arm64.whl (22.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.7/22.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.5 (from pinecone-datasets==0.5.0rc2)\n",
            "  Downloading pydantic-1.10.9-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting s3fs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
            "Collecting numpy>=1.17 (from datasets==2.12.0)\n",
            "  Downloading numpy-1.24.3-cp39-cp39-macosx_11_0_arm64.whl (13.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0)\n",
            "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "Collecting requests>=2.19.0 (from datasets==2.12.0)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Collecting tqdm>=4.62.1 (from datasets==2.12.0)\n",
            "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Collecting xxhash (from datasets==2.12.0)\n",
            "  Using cached xxhash-3.2.0-cp39-cp39-macosx_11_0_arm64.whl (31 kB)\n",
            "Collecting multiprocess (from datasets==2.12.0)\n",
            "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "Collecting aiohttp (from datasets==2.12.0)\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-macosx_11_0_arm64.whl (338 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.3/338.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets==2.12.0)\n",
            "  Using cached huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "Requirement already satisfied: packaging in /Users/roymiara/Pinecone/examples/.venv/lib/python3.9/site-packages (from datasets==2.12.0) (23.1)\n",
            "Collecting responses<0.19 (from datasets==2.12.0)\n",
            "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting pyyaml>=5.1 (from datasets==2.12.0)\n",
            "  Using cached PyYAML-6.0-cp39-cp39-macosx_11_0_arm64.whl (173 kB)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers==2.2.2)\n",
            "  Using cached transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "Collecting torch>=1.6.0 (from sentence-transformers==2.2.2)\n",
            "  Downloading torch-2.0.1-cp39-none-macosx_11_0_arm64.whl (55.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchvision (from sentence-transformers==2.2.2)\n",
            "  Downloading torchvision-0.15.2-cp39-cp39-macosx_11_0_arm64.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn (from sentence-transformers==2.2.2)\n",
            "  Downloading scikit_learn-1.2.2-cp39-cp39-macosx_12_0_arm64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from sentence-transformers==2.2.2)\n",
            "  Downloading scipy-1.10.1-cp39-cp39-macosx_12_0_arm64.whl (28.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.9/28.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
            "\u001b[?25hCollecting nltk (from sentence-transformers==2.2.2)\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting sentencepiece (from sentence-transformers==2.2.2)\n",
            "  Downloading sentencepiece-0.1.99-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting attrs>=17.3.0 (from aiohttp->datasets==2.12.0)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets==2.12.0)\n",
            "  Using cached charset_normalizer-3.1.0-cp39-cp39-macosx_11_0_arm64.whl (122 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.12.0)\n",
            "  Downloading multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.12.0)\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.12.0)\n",
            "  Downloading yarl-1.9.2-cp39-cp39-macosx_11_0_arm64.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.12.0)\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.12.0)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: decorator>4.1.2 in /Users/roymiara/Pinecone/examples/.venv/lib/python3.9/site-packages (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2) (5.1.1)\n",
            "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gcsfs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached gcsfs-2023.4.0-py2.py3-none-any.whl (26 kB)\n",
            "  Using cached gcsfs-2023.3.0-py2.py3-none-any.whl (26 kB)\n",
            "  Using cached gcsfs-2023.1.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting fsspec[http]>=2021.11.1 (from datasets==2.12.0)\n",
            "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "Collecting google-auth>=1.2 (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached google_auth-2.19.1-py2.py3-none-any.whl (181 kB)\n",
            "Collecting google-auth-oauthlib (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting google-cloud-storage (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached google_cloud_storage-2.9.0-py2.py3-none-any.whl (113 kB)\n",
            "Collecting filelock (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0)\n",
            "  Using cached filelock-3.12.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/roymiara/Pinecone/examples/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/roymiara/Pinecone/examples/.venv/lib/python3.9/site-packages (from pandas<3.0.0,>=2.0.0->pinecone-datasets==0.5.0rc2) (2.8.2)\n",
            "Collecting pytz>=2020.1 (from pandas<3.0.0,>=2.0.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas<3.0.0,>=2.0.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets==2.12.0)\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets==2.12.0)\n",
            "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets==2.12.0)\n",
            "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "Collecting aiobotocore~=2.5.0 (from s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached aiobotocore-2.5.0-py3-none-any.whl (72 kB)\n",
            "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc2)\n",
            "  Using cached s3fs-2023.5.0-py3-none-any.whl (28 kB)\n",
            "Collecting sympy (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Collecting networkx (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "Collecting jinja2 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading regex-2023.6.3-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading safetensors-0.3.1-cp39-cp39-macosx_12_0_arm64.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting click (from nltk->sentence-transformers==2.2.2)\n",
            "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
            "Collecting joblib (from nltk->sentence-transformers==2.2.2)\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers==2.2.2)\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->sentence-transformers==2.2.2)\n",
            "  Downloading Pillow-9.5.0-cp39-cp39-macosx_11_0_arm64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting botocore<1.29.77,>=1.29.76 (from aiobotocore~=2.5.0->s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached botocore-1.29.76-py3-none-any.whl (10.4 MB)\n",
            "Collecting wrapt>=1.10.10 (from aiobotocore~=2.5.0->s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached wrapt-1.15.0-cp39-cp39-macosx_11_0_arm64.whl (36 kB)\n",
            "Collecting aioitertools>=0.5.1 (from aiobotocore~=2.5.0->s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /Users/roymiara/Pinecone/examples/.venv/lib/python3.9/site-packages (from google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2) (1.16.0)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets==2.12.0)\n",
            "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
            "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Collecting google-resumable-media>=2.3.2 (from google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
            "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting googleapis-common-protos<2.0dev,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0 (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached google_crc32c-1.5.0-cp39-cp39-macosx_10_9_universal2.whl (32 kB)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc2)\n",
            "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pytz, mpmath, xxhash, wrapt, urllib3, tzdata, tqdm, threadpoolctl, sympy, regex, pyyaml, pydantic, pyasn1, protobuf, polars, pinecone-client, pillow, oauthlib, numpy, networkx, multidict, MarkupSafe, joblib, jmespath, idna, google-crc32c, fsspec, frozenlist, filelock, dill, click, charset-normalizer, certifi, cachetools, attrs, async-timeout, aioitertools, yarl, scipy, rsa, requests, pyasn1-modules, pyarrow, pandas, nltk, multiprocess, jinja2, googleapis-common-protos, google-resumable-media, botocore, aiosignal, torch, scikit-learn, responses, requests-oauthlib, huggingface-hub, google-auth, aiohttp, transformers, torchvision, google-auth-oauthlib, google-api-core, aiobotocore, sentence-transformers, s3fs, google-cloud-core, datasets, google-cloud-storage, gcsfs, pinecone-datasets\n",
            "Successfully installed MarkupSafe-2.1.3 aiobotocore-2.5.0 aiohttp-3.8.4 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 botocore-1.29.76 cachetools-5.3.1 certifi-2023.5.7 charset-normalizer-3.1.0 click-8.1.3 datasets-2.12.0 dill-0.3.6 filelock-3.12.1 frozenlist-1.3.3 fsspec-2023.5.0 gcsfs-2023.5.0 google-api-core-2.11.0 google-auth-2.19.1 google-auth-oauthlib-1.0.0 google-cloud-core-2.3.2 google-cloud-storage-2.9.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.0 huggingface-hub-0.15.1 idna-3.4 jinja2-3.1.2 jmespath-1.0.1 joblib-1.2.0 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.14 networkx-3.1 nltk-3.8.1 numpy-1.24.3 oauthlib-3.2.2 pandas-2.0.2 pillow-9.5.0 pinecone-client-3.0.0rc2 pinecone-datasets-0.5.0rc2 polars-0.16.18 protobuf-3.19.6 pyarrow-12.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pydantic-1.10.9 pytz-2023.3 pyyaml-6.0 regex-2023.6.3 requests-2.31.0 requests-oauthlib-1.3.1 responses-0.18.0 rsa-4.9 s3fs-2023.5.0 safetensors-0.3.1 scikit-learn-1.2.2 scipy-1.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.1.0 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 tqdm-4.65.0 transformers-4.30.1 tzdata-2023.3 urllib3-1.26.16 wrapt-1.15.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \\\n",
        "  pinecone-datasets==0.5.0rc2 \\\n",
        "  datasets==2.12.0 \\\n",
        "  sentence-transformers==2.2.2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "🚨 _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hrSfFiIC5roI"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kujS_e8s55oJ"
      },
      "source": [
        "The dataset preparation process requires a few steps:\n",
        "\n",
        "1. We download the Quora dataset from Hugging Face Datasets.\n",
        "\n",
        "2. The text content of the dataset is embedded into vectors.\n",
        "\n",
        "3. We create a Pinecone Dataset and save it.\n",
        "\n",
        "4. We upload the dataset to Pinecone.\n",
        "\n",
        "We will see how steps `1 - 4` are done in this section, but we won't implement `2` across the whole dataset until we reach the *upsert loop* as we will iteratively perform these two steps.\n",
        "\n",
        "In either case, this can take some time. If you'd rather skip the data preparation step and get straight to upserts and testing the semantic search functionality, you should \n",
        "refer to the [**fast notebook**](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/semantic-search-fast.ipynb). The uses a premade dataset and is ready to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeJPWu9P7EtR",
        "outputId": "2323c6b2-5feb-4601-e843-1dc04e272008"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/roymiara/Pinecone/examples/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading builder script: 100%|██████████| 2.38k/2.38k [00:00<00:00, 9.31MB/s]\n",
            "Downloading metadata: 100%|██████████| 1.13k/1.13k [00:00<00:00, 9.25MB/s]\n",
            "Downloading readme: 100%|██████████| 5.69k/5.69k [00:00<00:00, 27.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset quora/default to /Users/roymiara/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 58.2M/58.2M [00:05<00:00, 10.9MB/s]\n",
            "Downloading data files: 100%|██████████| 1/1 [00:05<00:00,  5.88s/it]\n",
            "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 983.19it/s]\n",
            "                                                                                         "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset quora downloaded and prepared to /Users/roymiara/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['questions', 'is_duplicate'],\n",
              "    num_rows: 80000\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('quora', split='train[240000:320000]')\n",
        "dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ngFHND1nQQU2"
      },
      "source": [
        "The dataset contains ~400K pairs of natural language questions from Quora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsA67WpW7El4",
        "outputId": "5b69152a-3809-453d-ae9e-0bddc66db2eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'questions': [{'id': [207550, 351729],\n",
              "   'text': ['What is the truth of life?', \"What's the evil truth of life?\"]},\n",
              "  {'id': [33183, 351730],\n",
              "   'text': ['Which is the best smartphone under 20K in India?',\n",
              "    'Which is the best smartphone with in 20k in India?']},\n",
              "  {'id': [351731, 351732],\n",
              "   'text': ['Steps taken by Canadian government to improve literacy rate?',\n",
              "    'Can I send homemade herbal hair oil from India to US via postal or private courier services?']},\n",
              "  {'id': [37799, 94186],\n",
              "   'text': ['What is a good way to lose 30 pounds in 2 months?',\n",
              "    'What can I do to lose 30 pounds in 2 months?']},\n",
              "  {'id': [351733, 351734],\n",
              "   'text': ['Which of the following most accurately describes the translation of the graph y = (x+3)^2 -2 to the graph of y = (x -2)^2 +2?',\n",
              "    'How do you graph x + 2y = -2?']}],\n",
              " 'is_duplicate': [False, True, False, True, False]}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[:5]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Zy8zoeQmRZ"
      },
      "source": [
        "Whether or not the questions are duplicates is not so important, all we need for this example is the text itself. We can extract them all into a single `questions` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heGUpy_37Eis",
        "outputId": "a9b61af4-6595-44fc-bd35-7a7501b26123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If Allah is merciful then why would He burn sinned humans with fire for eternity in hell? Cant' He show mercy?\n",
            "What percentage of transgender women are sexually attracted to women as opposed to men, or both men and women?\n",
            "What is the advantage of polarized sunglasses?\n",
            "What are good gift ideas for a dad in his late 40s?\n",
            "Who are the people still choosing to vote for Donald Trump and why do they want to vote for him (other than because he's the Republican nominee)?\n",
            "136057\n"
          ]
        }
      ],
      "source": [
        "questions = []\n",
        "\n",
        "for record in dataset['questions']:\n",
        "    questions.extend(record['text'])\n",
        "  \n",
        "# remove duplicates\n",
        "questions = list(set(questions))\n",
        "print('\\n'.join(questions[:5]))\n",
        "print(len(questions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4BknpfucRkkm"
      },
      "source": [
        "With our questions ready to go we can move on to demoing steps **2** and **3** above.\n",
        "\n",
        "### Building Embeddings\n",
        "\n",
        "To create our embeddings we will us the `MiniLM-L6` sentence transformer model. This is a very efficient semantic similarity embedding model from the `sentence-transformers` library. We initialize it like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxcGjb9GSEqA",
        "outputId": "e3b269d0-c7f4-41d8-85ad-5a94a0b79d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using cpu. This is much slower than using a CUDA-enabled GPU. If on Colab you can change this by clicking Runtime > Change runtime type > GPU.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              "  (2): Normalize()\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device != 'cuda':\n",
        "    print(f\"You are using {device}. This is much slower than using \"\n",
        "          \"a CUDA-enabled GPU. If on Colab you can change this by \"\n",
        "          \"clicking Runtime > Change runtime type > GPU.\")\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iy2itPb0S5js"
      },
      "source": [
        "There are *three* interesting bits of information in the above model printout. Those are:\n",
        "\n",
        "* `max_seq_length` is `256`. That means that the maximum number of tokens (like words) that can be encoded into a single vector embedding is `256`. Anything beyond this *must* be truncated.\n",
        "\n",
        "* `word_embedding_dimension` is `384`. This number is the dimensionality of vectors output by this model. It is important that we know this number later when initializing our Pinecone vector index.\n",
        "\n",
        "* `Normalize()`. This final normalization step indicates that all vectors produced by the model are normalized. That means that models that we would typical measure similarity for using *cosine similarity* can also make use of the *dotproduct* similarity metric. In fact, with normalized vectors *cosine* and *dotproduct* are equivalent.\n",
        "\n",
        "Moving on, we can create a sentence embedding using this model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyzoJEsAULOe",
        "outputId": "a1ea6149-4b2d-4dfb-e984-b067fb9980d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 384)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = 'which city is the most populated in the world?'\n",
        "\n",
        "xq = model.encode(query)\n",
        "xq.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qVZi8xevUWM6"
      },
      "source": [
        "Encoding this single sentence leaves us with a `384` dimensional sentence embedding (aligned to the `word_embedding_dimension` above).\n",
        "\n",
        "To prepare this for `upsert` to Pinecone, all we do is this:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wiXig_rHV2Wz"
      },
      "source": [
        "Later when we do upsert our data to Pinecone, we will be doing so in batches. Meaning `vectors` will be a list of `(id, embedding, metadata)` tuples."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YUd1VGg6i108"
      },
      "source": [
        "Now we upsert the data, we will do this in batches of `128`.\n",
        "\n",
        "_**Note:** On Google Colab with GPU expected runtime is ~7 minutes. If using CPU this will be significantly longer. If you'd like to get this running faster refer to the [fast notebook](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/semantic-search-fast.ipynb)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first create list for ids, embeddings and metadata\n",
        "ids = []\n",
        "embeddings = []\n",
        "metadata = []\n",
        "\n",
        "batch_size = 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "0e5dc9a271184100a92cd6b373ab8e7d",
            "8396bfd36c2b4869a0b4680587b604f1",
            "d714f8b3ebca4195b747ab18d196b88b",
            "56266bc7062541f5ba042848205270ef",
            "78fab28034dd428c9f49a5653fd003e5",
            "d8ca1c42783f41a8bd17491073883fdd",
            "cffd5e66b82344b0946630eb320ca5b4",
            "b63c0eee680f4708a4c68a4746cb21a4",
            "12e8c994f3ac4f88ae5387c72b4d8cc7",
            "9ea70e9ea5fb4040a080b941950b0fef",
            "b30a98ddfc1841e381937571edcfae71"
          ]
        },
        "id": "RhR6WOi1huXZ",
        "outputId": "ef9f74ef-2ae3-4eb3-cef4-6814e98861a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# note doing it on 1000 questiosn \n",
        "for i in tqdm(range(0, len(questions[:1024]), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(i+batch_size, len(questions))\n",
        "    # create IDs batch\n",
        "    _ids = [str(x) for x in range(i, i_end)]\n",
        "    # create metadata batch\n",
        "    _metadata = [{'text': text} for text in questions[i:i_end]]\n",
        "    # create embeddings\n",
        "    _embeddings = model.encode(questions[i:i_end])\n",
        "    # create records list for upsert\n",
        "\n",
        "    ids.extend(_ids)\n",
        "    embeddings.extend(_embeddings)\n",
        "    metadata.extend(_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[0.079809085, 0.13530786, -0.024871217, 0.0126...</td>\n",
              "      <td>{'text': 'If Allah is merciful then why would ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[0.08731179, -0.043924116, -0.07815887, 0.0399...</td>\n",
              "      <td>{'text': 'What percentage of transgender women...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[-0.044097595, 0.012648403, 0.007437395, 0.013...</td>\n",
              "      <td>{'text': 'What is the advantage of polarized s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[0.019823564, 0.062449012, 0.015589851, -0.011...</td>\n",
              "      <td>{'text': 'What are good gift ideas for a dad i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[0.056418877, -0.089993075, 0.06808353, -0.019...</td>\n",
              "      <td>{'text': 'Who are the people still choosing to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>1019</td>\n",
              "      <td>[-0.033038545, 0.08281174, -0.055870146, 0.061...</td>\n",
              "      <td>{'text': 'How was hemoglobin discovered? Who d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020</th>\n",
              "      <td>1020</td>\n",
              "      <td>[0.018484745, 0.062107757, 0.034533918, 0.0269...</td>\n",
              "      <td>{'text': 'Why did Steve Jobs drop out of colle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1021</th>\n",
              "      <td>1021</td>\n",
              "      <td>[0.005033609, -0.084230006, -0.013950559, 0.01...</td>\n",
              "      <td>{'text': 'My wife and I fight a lot and I need...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1022</th>\n",
              "      <td>1022</td>\n",
              "      <td>[0.054642506, -0.06619325, -0.05927356, -0.035...</td>\n",
              "      <td>{'text': 'I'm worried about my relationship. S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1023</th>\n",
              "      <td>1023</td>\n",
              "      <td>[-0.025504842, 0.008972524, -0.070964225, -0.0...</td>\n",
              "      <td>{'text': 'Why naidus and Reddys dominate in po...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1024 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                             values  \\\n",
              "0        0  [0.079809085, 0.13530786, -0.024871217, 0.0126...   \n",
              "1        1  [0.08731179, -0.043924116, -0.07815887, 0.0399...   \n",
              "2        2  [-0.044097595, 0.012648403, 0.007437395, 0.013...   \n",
              "3        3  [0.019823564, 0.062449012, 0.015589851, -0.011...   \n",
              "4        4  [0.056418877, -0.089993075, 0.06808353, -0.019...   \n",
              "...    ...                                                ...   \n",
              "1019  1019  [-0.033038545, 0.08281174, -0.055870146, 0.061...   \n",
              "1020  1020  [0.018484745, 0.062107757, 0.034533918, 0.0269...   \n",
              "1021  1021  [0.005033609, -0.084230006, -0.013950559, 0.01...   \n",
              "1022  1022  [0.054642506, -0.06619325, -0.05927356, -0.035...   \n",
              "1023  1023  [-0.025504842, 0.008972524, -0.070964225, -0.0...   \n",
              "\n",
              "                                               metadata  \n",
              "0     {'text': 'If Allah is merciful then why would ...  \n",
              "1     {'text': 'What percentage of transgender women...  \n",
              "2     {'text': 'What is the advantage of polarized s...  \n",
              "3     {'text': 'What are good gift ideas for a dad i...  \n",
              "4     {'text': 'Who are the people still choosing to...  \n",
              "...                                                 ...  \n",
              "1019  {'text': 'How was hemoglobin discovered? Who d...  \n",
              "1020  {'text': 'Why did Steve Jobs drop out of colle...  \n",
              "1021  {'text': 'My wife and I fight a lot and I need...  \n",
              "1022  {'text': 'I'm worried about my relationship. S...  \n",
              "1023  {'text': 'Why naidus and Reddys dominate in po...  \n",
              "\n",
              "[1024 rows x 3 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating a dataframe\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'id': ids, 'values': embeddings, 'metadata': metadata})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a Pinecone dataset\n",
        "from pinecone_datasets import Dataset as PineconeDataset, DatasetMetadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_metadata = DatasetMetadata(\n",
        "    **{\n",
        "        'name': 'quora_all-MiniLM-L6-bm25',\n",
        "        'created_at': '2023-02-17 14:17:01.481785',\n",
        "        'documents': 522931,\n",
        "        'queries': 0,\n",
        "        'source': 'https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs',\n",
        "        'license': None,\n",
        "        'bucket': 'gs://pinecone-datasets-dev',\n",
        "        'task': 'similar questions',\n",
        "        'dense_model': {\n",
        "            'name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "            'tokenizer': None,\n",
        "            'dimension': 384\n",
        "        },\n",
        "        'sparse_model': None,\n",
        "        'description': None,\n",
        "        'tags': None,\n",
        "        'args': None\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "pds = PineconeDataset.from_pandas(df, metadata=dataset_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>sparse_values</th>\n",
              "      <th>metadata</th>\n",
              "      <th>blob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[0.079809085, 0.13530786, -0.024871217, 0.0126...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'text': 'If Allah is merciful then why would ...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[0.08731179, -0.043924116, -0.07815887, 0.0399...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'text': 'What percentage of transgender women...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[-0.044097595, 0.012648403, 0.007437395, 0.013...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'text': 'What is the advantage of polarized s...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[0.019823564, 0.062449012, 0.015589851, -0.011...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'text': 'What are good gift ideas for a dad i...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[0.056418877, -0.089993075, 0.06808353, -0.019...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'text': 'Who are the people still choosing to...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id                                             values sparse_values  \\\n",
              "0  0  [0.079809085, 0.13530786, -0.024871217, 0.0126...          None   \n",
              "1  1  [0.08731179, -0.043924116, -0.07815887, 0.0399...          None   \n",
              "2  2  [-0.044097595, 0.012648403, 0.007437395, 0.013...          None   \n",
              "3  3  [0.019823564, 0.062449012, 0.015589851, -0.011...          None   \n",
              "4  4  [0.056418877, -0.089993075, 0.06808353, -0.019...          None   \n",
              "\n",
              "                                            metadata  blob  \n",
              "0  {'text': 'If Allah is merciful then why would ...  None  \n",
              "1  {'text': 'What percentage of transgender women...  None  \n",
              "2  {'text': 'What is the advantage of polarized s...  None  \n",
              "3  {'text': 'What are good gift ideas for a dad i...  None  \n",
              "4  {'text': 'Who are the people still choosing to...  None  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pds.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/roymiara/Pinecone/examples/.venv/lib/python3.9/site-packages/pinecone_datasets/dataset.py:323: UserWarning: Queries are empty, not saving queries\n",
            "  warnings.warn(\"Queries are empty, not saving queries\")\n"
          ]
        }
      ],
      "source": [
        "# saving dataset for later\n",
        "pds.to_path('./tmp/quora_all-MiniLM-L6-bm25')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "ename": "ConnectionError",
          "evalue": "Failed to connect to Pinecone's controller on region YOUR_ENVIRONMENT. Please verify client configuration: API key, region and project_id. See more info: https://docs.pinecone.io/docs/quickstart#2-get-and-verify-your-pinecone-api-key\nUnderlying Error: error sending request for url (https://controller.your_environment.pinecone.io/actions/whoami): error trying to connect: dns error: failed to lookup address information: nodename nor servname provided, or not known",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mPINECONE_API_KEY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYOUR_API_KEY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mPINECONE_ENVIRONMENT\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYOUR_ENVIRONMENT\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pds\u001b[39m.\u001b[39;49mto_index(\u001b[39m\"\u001b[39;49m\u001b[39mseamntic-search\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, concurrency\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, create_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, metadata_config\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mindexed\u001b[39;49m\u001b[39m\"\u001b[39;49m: []})\n",
            "File \u001b[0;32m~/Pinecone/examples/.venv/lib/python3.9/site-packages/pinecone_datasets/dataset.py:410\u001b[0m, in \u001b[0;36mDataset.to_index\u001b[0;34m(self, index_name, bath_size, concurrency, create_index, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPINECONE_API_KEY environment variable must be set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    409\u001b[0m \u001b[39m# create client\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m pinecone \u001b[39m=\u001b[39m Client(api_key\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mPINECONE_API_KEY\u001b[39;49m\u001b[39m\"\u001b[39;49m], region\u001b[39m=\u001b[39;49mregion)\n\u001b[1;32m    412\u001b[0m pinecone_index_list \u001b[39m=\u001b[39m pinecone\u001b[39m.\u001b[39mlist_indexes()\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m create_index:\n\u001b[1;32m    415\u001b[0m     \u001b[39m# make sure the index does not exist\u001b[39;00m\n",
            "\u001b[0;31mConnectionError\u001b[0m: Failed to connect to Pinecone's controller on region YOUR_ENVIRONMENT. Please verify client configuration: API key, region and project_id. See more info: https://docs.pinecone.io/docs/quickstart#2-get-and-verify-your-pinecone-api-key\nUnderlying Error: error sending request for url (https://controller.your_environment.pinecone.io/actions/whoami): error trying to connect: dns error: failed to lookup address information: nodename nor servname provided, or not known"
          ]
        }
      ],
      "source": [
        "# upserting datsaet to Pinecone\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "os.environ[\"PINECONE_ENVIRONMENT\"] = \"YOUR_ENVIRONMENT\"\n",
        "\n",
        "pds.to_index(\"seamntic-search\", batch_size=300, concurrency=16, create_index=True, metadata_config={\"indexed\": []})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PMMJSu_DbRx0"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e5dc9a271184100a92cd6b373ab8e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8396bfd36c2b4869a0b4680587b604f1",
              "IPY_MODEL_d714f8b3ebca4195b747ab18d196b88b",
              "IPY_MODEL_56266bc7062541f5ba042848205270ef"
            ],
            "layout": "IPY_MODEL_78fab28034dd428c9f49a5653fd003e5"
          }
        },
        "12e8c994f3ac4f88ae5387c72b4d8cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56266bc7062541f5ba042848205270ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea70e9ea5fb4040a080b941950b0fef",
            "placeholder": "​",
            "style": "IPY_MODEL_b30a98ddfc1841e381937571edcfae71",
            "value": " 1063/1063 [02:48&lt;00:00,  6.41it/s]"
          }
        },
        "78fab28034dd428c9f49a5653fd003e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8396bfd36c2b4869a0b4680587b604f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ca1c42783f41a8bd17491073883fdd",
            "placeholder": "​",
            "style": "IPY_MODEL_cffd5e66b82344b0946630eb320ca5b4",
            "value": "100%"
          }
        },
        "9ea70e9ea5fb4040a080b941950b0fef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b30a98ddfc1841e381937571edcfae71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b63c0eee680f4708a4c68a4746cb21a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cffd5e66b82344b0946630eb320ca5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d714f8b3ebca4195b747ab18d196b88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63c0eee680f4708a4c68a4746cb21a4",
            "max": 1063,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12e8c994f3ac4f88ae5387c72b4d8cc7",
            "value": 1063
          }
        },
        "d8ca1c42783f41a8bd17491073883fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
